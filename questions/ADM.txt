What is a difference between large-scale data frameworks (e.g Hadoop) and large-scale DBMS? * frameworks don't provide a logical level * framework focus on data representation and querying * frameworks are tailored to transactional processing * 1
Connect RDBMS (1) and LSDBMS (2) featues: application database, semi-structured schema, flat data, joins are an issue and normalization is no more a reference, declarative lang (SQL) * 2, 1, 1, 2, 1 * 2, 2, 1, 2, 1 * 2, 2, 1, 1, 1 * 2
In ?? DB we start from the ?? to model our data, and all the (any) app that will need that data will come later (with different views) * application, domain knowledge * application, relational schema * integration, domain knowledge * 3
An ?? is tailored to account for the design of a niche of apps, with data exchange between apps based on specific format (e.g JSON) and with the chance of putting ACL and integrity checks in the ?? * application DB, app code * application DB, DB code * integration DB, app code * 1
?? are inefficient for large volumes of data (if data are partitioned in 2 nodes, it'll be made 2x2=4 times), so in app DB we use query-based design * products * joins * CRUD operations * 2
?? is a term used to describe the problem that arises when two systems or components that are supposed to work together have different data models or structures that make communication difficult * Impedence mismatch * Inconsistencies * both * 1
Graph based systems are tailored to app where complexity of data is the most important point, while Aggregate oriented systems are designed to deal with common large scale issues * TRUE * FALSE, the opposite * completely FALSE * 1
?? architectures are often P2P, designed for transactional (read/write intensive) scenarios, with hash-based or range-based partitioning for efficient resource usage, leader-less replication, eventual consistency, high and tunable availability, high fault tolerance (often with no master, P2P ring), usually no ACID transactions, and following the AP or CP theorem in the CAP theorem. * Graph based * Aggregate-oriented * None * 2
The notion of aggregate (a data unit with a complex structure)...: * impact the logical level * impact the physical level * both * 3
A ?? exists only if a system that uses that level also exists, and it is a set of constructs that helps us in representing entities and associations corresponding to a general/reference domain * conceptual * logical * None * 2
An aggregate is the unit of interaction with the data store, so: * you have to partition them * you can partition them * you can't partition them * 3
In Application DB there's no such thing as ??, because the logical schema was built with respect to the needs of the few apps that will use it * External (Views) Level * Logical Level * SQL Middleware * 1
?? define how data is logically and physically grouped, impacting search efficiency, concurrency control, and consistency, with NoSQL databases typically supporting atomic manipulation of a single aggregate at a time to avoid inconsistent reads during multi-aggregate updates * Logical views * Aggregate properties * Graph views * 2
These aggregate-orieted logical data models are in what order? Key value, Document-oriented, Column family * increasing data size * increasing data complexity * decreasing data complexity * 2
In aggregate-orieted logical data models, at the physical level, the key is the ??: different aggregates associated with the same key are stored in the same node * partitioning value * JSON required fields * the (non necessarily unique) key of an entity * 1
Without considering a specific aggr.-oriented system: starting from a conceptual schema and a ?? we want to translate them in a meta-logical schema (using JSON ??) * workload, schemas * workload, tables * logical, tables * 1
Aggregate-oriented logical design principles include minimizing ?? by reducing collections, reducing redundancy with complex attributes like arrays or objects, and prioritizing selections over ?? attributes for efficiency * joins, complex * projections, simple * joins, simple * 3
The methodology involves taking an ?? and ?? as input, formally modeling each query, annotating the ER schema with ?? information, and then generating the aggregate-oriented logical schema based on the annotations, which serves as the final output * ER schema, workload, query * query, ER schema, workload * both are ok * 1
The aggregate key is often used as ??* foreign key * partitioning key * parititioning rule * 2
in partitioning with ?? you have that H(K) = i? * chord é cassandra * i betweeen [0, 2^n] *   * 1 i betweeen [0, 2^n-1] * 2
Each node is associated with an ID, which is used for hashing (details on the hashing mechanism are omitted). Aggregates, whose keys are hashed to a value k, are assigned to the first node whose ID is either equal to or immediately follows k in the identifier space (the successor node of k). If S and S' are two adjacent nodes in the ring (in a clockwise direction), all keys in the range ?? are mapped to node S. * (h(S), h(S')] * [h(S), h(S')) * [h(S), h(S')] * 1
In chord, apart from the logN choice, what are the others? * Each node records its successor on the ring * Full duplication of the hash directory at each node * both * 3
In the Chord Ring, how is item x stored on a node? * Corresponding to a random identifier * Matching its specific hash value * Determined by the node's predecessor * 2
What's pros/cons of having chord nodes knowing only the next? * in case of addition only the previous has to be updated * O(N) * first is pro, second is con * 3
How does Chord achieve accelerated lookup in its routing mechanism? * By implementing random node connections * Through the creation of exponential shortcuts (logN lookup) * By establishing direct links to all successor nodes * 2
In Chord's greedy routing, what approach is taken at each step of the lookup process? * Following the closest, but before, finger to the destination * Prioritizing communication with all successor nodes * Randomly selecting a successor at each step * 1
What ensure these CHORD steps: When a node p wants to join, it uses a contact node p' to: initialize p's routing table by locating p's friends, update existing nodes' routing tables via a background "stabilization" protocol that maintains Chord’s finger tables and successor pointers, and transfer items k from its successor where h(k) ≤ h(p) * scalability * replication * fault-tolerance * 1
What ensure this CHORD consistent hashing step: assign keys to m successor nodes * scalability * replication * fault-tolerance * 2
What BASE stands for? * Basically Available, Soft State, Eventual Consistency * Biased Acid Sium Enterprise * Basically Available, Soon or Later, Eventual Consistency * 1
In a ?? data model, each data instance is stored as an opaque pair without a defined structure, allowing high flexibility in data content, with aggregate visibility limited to the application level (logically groupable by collections) * key-value * document-oriented * graph-based * 1
In key-value data models, keys uniquely identify data for retrieval and partitioning, enabling direct access to entire aggregates but not to specific fields, with relationships navigated through sequential lookups, often within a ??-specific namespace * key * collection * aggregate * 2
In a document-oriented data model, each data instance is a (key, value) pair where the key identifies the document, and the value is a structured, accessible aggregate of nested <name, nested-document> pairs, allowing visibility and access at * application level, as key-value ones * both logical and application levels * physical and application level * 2
In ?-oriented data models, ?s are self-describing, hierarchical structures (often in XML, JSON, or BSON) with flexible schemas, allowing dynamic addition of new attributes without predefined schema requirements * key-value * graph * document * 3
In document-oriented data models, collections are logical groupings of documents (key-value pairs) with flexible structures, allowing varied fields across documents, though documents within a collection generally share similar purposes * true * false * true but in MongoDB optional JSON schema validation can enforce data consistency * 3
In ??, an instance hosts multiple databases, each with collections (like tables) where documents are stored (via db.collection.insertOne(document)) with a unique system-generated _id identifier * MongoDB * Cassandra * Redis * 1
Other difference(s) among key-value and document-based? * In doc-based partition key and identifier can be different * In doc-based aggregates can be directly retrieved by specifying values for attributes in the key OR for nested attributes; because the structure of the aggregate is exposed * both * 3
In MongoDB, while unique _id identifiers allow document referencing ??, partition keys play a role in data storage organization * in the context of retrieval * without affecting retrieval * with the chance of affecting retrieval * 2
Interaction with ?? abstracts query language through basic operations for document retrieval and manipulation (like get, put, set, and remove), allowing for queries on nested documents, additional search constraints, partial retrieval through projection, and requiring collection names in each operation when collections are used * Redis * Cassandra * MongoDB * 3
?? supports the aggregation framework and the ?? construct for performing joins, requiring explicit instructions for the system on what to match, but this involves two separate data accesses at potentially different nodes, making it less efficient than queries on embedded relationships * MongoDB, $lookup * Cassandra, $lookup * Cassandra, $join * 1
? is a document-based database with support for aggregation, transactional and analytical scenarios, hash and range-based sharding, primary and secondary indexes, master-slave and replica set replication, strong and eventual (at replicas) consistency, availability through read/write mediation, fault tolerance via replica sets, multi-document ACID transactions and aligns with the ? model of the CAP theorem * MongoDB, AP * MongoDB, CP * Cassandra, AP * 2
Remember that the following practical topics aren't here: 3 steps of creating aggregates from ER and workload + mongoDB queries + ... * OK * SIUM * SIUM * 1
db.videos.find({},{title:1}) is a? * projection only with title * projection only with title and id * projection with title and selection of everything * 2
db.collection.find( <query>, <projection> ) return a ?? to handle the result set (imposing limits, sorting, ecc) * cursor * callback * pointer * 1
match " db.W.aggregate([{$lookup: {from: Y, K: <field from the input docs>, Z: <field from the docs of the 'from' collection>, as: X}}]) " with " SELECT all, X FROM W WHERE X IN ( SELECT all FROM Y WHERE Z = <W.K> " * W = <collection to join>, Y = collection, K = localField, Z = <foreignField>, X = <output array field> * W = <collection to join>, Y = collection, K = <foreignField>, Z = <output array field>, X = localField * W = collection, Y = <collection to join>, K = localField, Z = <foreignField>, X = <output array field> * 3
db.orders.aggregate( {$?? : {_id: type, totalquantity: { $sum: quantity} } } ) * having * group * match * 2
MapReduce in ?? performs complex aggregation on key-value pairs using a map function, reduce function, and output collection, with support for automatic parallel processing across partitions (??) if the input is partitioned, and can partition the output collection using the _id field as the partition key if specified * MongoDB, shards * Cassandra, splits * MongoDB, splits * 1
All ?? operations in MongoDB are atomic on the level of a single ?? * write, document * read, aggregate * write, aggregate * 1
MongoDB automatically creates an index on the ? field and enables users to define additional indexes, including single-field, compound, unique, hash, and text indexes, to improve query performance and enforce data constraints; ? indexes take field order into account, while indexing array fields generates entries for each array element. MongoDB also provides a specialized text index to support text ? across documents * _id, compound, search * _id, hash, find * key, hash, search * 1
Ranged sharding is most efficient when the shard key is: * Large Cardinality & Low Frequency * Non-Monotonically Changing * both are true * 3
In ?? replica sets, nodes elect a primary (master) for handling all read and write operations, while secondary nodes asynchronously replicate the primary’s data and handle only ?? requests; if the primary fails, the secondaries vote to elect a new primary, ensuring fault tolerance and continuity despite potential node failures * PostgreSQL, write * MongoDB, read * Cassandra, read * 2
In MongoDB, ? transactions at the ? level are supported from version 4.0, requiring all operations to use the primary read preference and route to the same member; data changes remain invisible outside the transaction until it commits * atomic, multi-document * atomic, logical * flat, multi-document * 1
