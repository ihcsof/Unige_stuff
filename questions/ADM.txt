What is a difference between large-scale data frameworks (e.g Hadoop) and large-scale DBMS? * frameworks don't provide a logical level * framework focus on data representation and querying * frameworks are tailored to transactional processing * 1
Connect RDBMS (1) and LSDBMS (2) featues: application database, semi-structured schema, flat data, joins are an issue and normalization is no more a reference, declarative lang (SQL) * 2, 1, 1, 2, 1 * 2, 2, 1, 2, 1 * 2, 2, 1, 1, 1 * 2
In ?? DB we start from the ?? to model our data, and all the (any) app that will need that data will come later (with different views) * application, domain knowledge * 
application, relational schema * integration, domain knowledge * 3
An ?? is tailored to account for the design of a niche of apps, with data exchange between apps based on specific format (e.g JSON) and with the chance of putting ACL and integrity checks in the ?? * application DB, app code * application DB, DB code * integration DB, app code * 1
?? are inefficient for large volumes of data (if data are partitioned in 2 nodes, it'll be made 2x2=4 times), so in app DB we use query-based design * products * joins * CRUD operations * 2
?? is a term used to describe the problem that arises when two systems or components that are supposed to work together have different data models or structures that make communication difficult * Impedence mismatch * Inconsistencies * both * 1
Graph based systems are tailored to app where complexity of data is the most important point, while Aggregate oriented systems are designed to deal with common large scale issues * TRUE * FALSE, the opposite * completely FALSE * 1
?? architectures are often P2P, designed for transactional (read/write intensive) scenarios, with hash-based or range-based partitioning for efficient resource usage, leader-less replication, eventual consistency, high and tunable availability, high fault tolerance (often with no master, P2P ring), usually no ACID transactions, and following the AP or CP theorem in the CAP theorem. * Graph based * Aggregate-oriented * None * 2
The notion of aggregate (a data unit with a complex structure)...: * impact the logical level * impact the physical level * both * 3
A ?? exists only if a system that uses that level also exists, and it is a set of constructs that helps us in representing entities and associations corresponding to a general/reference domain * conceptual * logical * None * 2
An aggregate is the unit of interaction with the data store, so: * you have to partition them * you can partition them * you can't partition them * 3
In Application DB there's no such thing as ??, because the logical schema was built with respect to the needs of the few apps that will use it * External (Views) Level * Logical Level * SQL Middleware * 1
?? define how data is logically and physically grouped, impacting search efficiency, concurrency control, and consistency, with NoSQL databases typically supporting atomic manipulation of a single aggregate at a time to avoid inconsistent reads during multi-aggregate updates * Logical views * Aggregate properties * Graph views * 2
These aggregate-orieted logical data models are in what order? Key value, Document-oriented, Column family * increasing data size * increasing data complexity * decreasing data complexity * 2
In aggregate-orieted logical data models, at the physical level, the key is the ??: different aggregates associated with the same key are stored in the same node * partitioning value * JSON required fields * the (non necessarily unique) key of an entity * 1
Without considering a specific aggr.-oriented system: starting from a conceptual schema and a ?? we want to translate them in a meta-logical schema (using JSON ??) * workload, schemas * workload, tables * logical, tables * 1
Aggregate-oriented logical design principles include minimizing ?? by reducing collections, reducing redundancy with complex attributes like arrays or objects, and prioritizing selections over ?? attributes for efficiency * joins, complex * projections, simple * joins, simple * 3
The methodology involves taking an ?? and ?? as input, formally modeling each query, annotating the ER schema with ?? information, and then generating the aggregate-oriented logical schema based on the annotations, which serves as the final output * ER schema, workload, query * query, ER schema, workload * both are ok * 1
The aggregate key is often used as ??* foreign key * partitioning key * parititioning rule * 2
in partitioning with ?? you have that H(K) = i? * chord Ã© cassandra * i betweeen [0, 2^n] *   * 1 i betweeen [0, 2^n-1] * 2
Each node is associated with an ID, which is used for hashing (details on the hashing mechanism are omitted). Aggregates, whose keys are hashed to a value k, are assigned to the first node whose ID is either equal to or immediately follows k in the identifier space (the successor node of k). If S and S' are two adjacent nodes in the ring (in a clockwise direction), all keys in the range ?? are mapped to node S. * (h(S), h(S')] * [h(S), h(S')) * [h(S), h(S')] * 1
In chord, apart from the logN choice, what are the others? * Each node records its successor on the ring * Full duplication of the hash directory at each node * both * 3
In the Chord Ring, how is item x stored on a node? * Corresponding to a random identifier * Matching its specific hash value * Determined by the node's predecessor * 2
What's pros/cons of having chord nodes knowing only the next? * in case of addition only the previous has to be updated * O(N) * first is pro, second is con * 3
How does Chord achieve accelerated lookup in its routing mechanism? * By implementing random node connections * Through the creation of exponential shortcuts (logN lookup) * By establishing direct links to all successor nodes * 2
In Chord's greedy routing, what approach is taken at each step of the lookup process? * Following the closest, but before, finger to the destination * Prioritizing communication with all successor nodes * Randomly selecting a successor at each step * 1

How does Chord ensure fault tolerance within the system? * By periodically redistributing data among all nodes * Through a comprehensive backup and recovery protocol * By maintaining a list of successor nodes and regular stabilization procedures * 3
In what way does the Chord architecture handle the addition of new nodes to the system? * By a lookup followed by taking the right data from both previous and next node * By a lookup followed by taking the right data from next node, and telling the prev to update its cache * By initiating a comprehensive data exchange with multiple nodes * 2
What key issue does the fault tolerance mechanism in Chord aim to prevent? * Data loss due to node failures * Disruption of the successor chain when no links remain in a node * Excessive resource consumption when too much messages are exchanged * 2
