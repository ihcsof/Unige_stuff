According to the text, what defines a distributed system? * A single multi-core computer * A network with infinite bandwidth * A collection of autonomous computing elements * 3
Which of the following isn't one of the 8 fallacies of distributed computing mentioned in the text? * network is reliable * network is fast * Transport cost is zero * 2
Why might someone need a distributed system, according to the text? * To have a single administrator * To reduce performance * To ensure availability * 3
What is necessary for nodes in a distributed system to be coherent? * A global clock * Node failures * Synchronization * 3
According to the text, why might someone choose to implement a distributed system for their application? * To have complete control as a single entity * To reduce costs * To ensure availability even if some components fail * 3
What challenges does the text mention regarding making disparate machines in a distributed system coherent? * Managing group membership and authorizations * Synchronizing global clocks * Centralizing control * 1
What is a transaction in the context of this text? * A change in system configuration * A simultaneous modification of multiple systems * An independent modification in a system that stores data * 3
What are the ACID properties related to transactions? * Availabilty, Isolation, Durability, Consistency * Atomicity, Consistency, Isolation, Durability * Atomicity, Consistency, Integrity, Durability * 2
When discussing the ACID property of Atomicity, what does it mean when a transaction is treated as a single unit? * It can be interrupted and resumed later * It succeeds or fails as a whole * It can be divided into smaller units * 2
Which property ensures that even if transactions run concurrently, the system behaves as if they've been running sequentially? * Atomicity * Durability * Isolation * 3
What is the consequence of a network partition in a distributed system, as mentioned in the text? * Partially offline system until the partition is resolved * Inconsistent and stale data * you have a trade off between 1) and 2) * 3
Which of the following systems is mentioned as an example of a non-ACID system? * GIT (conflicts) * DNS * both * 3
When were the ACID properties implemented, even though the acronym was coined later? * In 1983 * In 1973 * In 1999 * 2
What does it mean when a system is said to have "partition tolerance" according to the CAP theorem? * The system remains consistent * The system keeps working even if network messages are dropped * Every request receives a non-error response * 2
How does the CAP theorem illustrate the trade-offs in distributed systems? * It shows that you can have all properties simultaneously * It shows that achieving one property may come at the expense of the others * It doesn't address trade-offs in distributed systems * 2
What is an example of a system mentioned in the text that can work with inconsistent functionality? * GIT (conflicts) * Relational databases * Email servers * 1
How does the text describe the behavior of a system when it faces a network partition? * The system guarantees data consistency * The system returns an error * The system loses partition tolerance * 3
In the context of distributed systems, what is the primary purpose of the ACID properties? * To ensure the system remains available at all times * To provide a framework for handling network partitions * To implement transactional consistency and durability * 3
When scaling vertically, what happens when a start-up needs a bigger server? * The cost remains manageable * The server capacity increases proportionally * The cost becomes unmanageable * 3
What is the primary challenge when scaling horizontally in distributed computing? * Coordination and scalability * Increased hardware costs * Lack of redundancy * 1
How does the distributed system ensure scalability in the context of read-intensive workloads? * By replicating all data on each server * By running all write commands on a single server * By executing write commands on all machines and read commands locally * 3
What is the primary challenge when implementing a shared log in a distributed system with a fail-stop failure model? * Delayed communication * Arbitrary message loss * Malicious behavior * 2
In the fail-stop failure model, what is the primary assumption about failed nodes? * They will eventually resume * They lose all their data * They will never resume * 1
What is the primary reason for using a majority-based approach in consensus algorithms? * To minimize the number of servers required * To guarantee that decisions are taken * To ensure there are no conflicting interests * 2
What is the primary focus of Paxos in the context of distributed systems? * Leader election * Cluster management * Achieving consensus * 3
In the context of consensus algorithms, why did Leslie Lamport create Paxos? * To show that Nancy Lynch and Barbara Liskov made mistakes in their system * To prove that achieving consensus in distributed systems is impossible * To demonstrate the correctness of Lynch and Liskov's system * 2
What is the primary challenge in implementing the Paxos algorithm correctly? * Ensuring there are no conflicting interests among participants * Achieving a fast consensus * Understanding and handling the details of the algorithm * 3
In the basic Paxos algorithm, how is a decision reached? * Through a majority vote * Through a centralized coordinator * Through a random selection * 1
What is the primary purpose of leader election in distributed systems? * To ensure all servers have the same data * To minimize the number of servers required * To determine which server can propose values in consensus algorithms * 3
In the fail-stop failure model, what happens if a proposer fails during the "accept" phase? * The decision is immediately finalized * Another proposer eventually takes over and finishes the job * The consensus process starts over from the beginning * 2
What is the primary benefit of the Multi-Paxos approach compared to the basic Paxos algorithm? * It reduces the number of servers required * It ensures faster consensus * It builds a full log of decisions * 3
What does each machine's state represent in the context of Deterministic State Machines? * The machine's location * The machine's configuration * The machine's communication protocol * 2
How is it determined whether two machines are in the same state in Deterministic State Machines? * If they have identical memory * If they receive the same input in the same order * If they have the same initial state * 2
In the context of read-intensive workloads, what is the primary advantage of a distributed system based on deterministic state machines? * Improved scalability due to distributed processing * Reduced hardware requirements * Higher consistency through centralized control * 1
What is the primary reason for having a majority-based approach in consensus algorithms like Paxos? * To minimize hardware costs * To ensure that decisions are taken only when there is unanimous agreement * To prevent malicious behavior * 2
What is the significance of having a majority of nodes agree on a decision in the Paxos algorithm? * It ensures there are no conflicting interests among participants * It guarantees that the decision is correct * It allows for fast consensus * 2
In a fail-stop scenario, what is the primary characteristic of messages exchanged in Paxos? * They may take arbitrary time * They are guaranteed to be delivered instantly * They are encrypted for security * 1
If you have CP of CAP you have: * error * hangs forever * old or random data * 1
If you have AP of CAP you have: * error * hangs forever * old or random data * 3
If you have CA of CAP you have: * error * hangs forever * old or random data * 2
Who is the creator of this circus? * kanye west * lorenzofoschi * filippo ricca * 2
You're doing great :O * right * wrong * wrong * 1
How many round trips has Basic Paxos? * 1 * 2 * 3 * 2
How many round trips has Multi Paxos? * 1 * 2 * 3 * 1
What happens when an Acceptor fails after accepting? * protocol fails completely * proposer didn't received accepts, so he proposes again * proposer received two accepts: majority is ok! * 3
What happens when an Acceptor fails before accepting? * We're 2 out of 3 (proposer + other acceptor): majority is ok! * We're 2 out of 3 (two proposers): majority is ok! * proposer has to repeat proposal again * 1
What happens when a Proposer fails in prepare phase (case seen in lesson)? * protocol fails * Someone, after some time, takes care of it and becomes a proposer * protocol waits for the proposer to re-appear * 2
What happens when a Proposer fails in accept phase (case seen in lesson)? * Someone, after some time, takes care of it and becomes a proposer * protocol waits for the proposer to re-appear * Specifically Node2 propose B but Node3 lets him know that A has already been proposed * 3
What happens when you can't accept something in Paxos? * You go with the smallest previous round number * You go with the highest previous round number * You go with the highest round number between the previous one and what you have received * 2
In Raft, what happens in split vote? * Timeout will take care of removing the stale * A random candidate is chosen as leader * Vote is denied and protocol fails * 1
What happens in Paxos if Node3, after having accepted proposal by Node1 that now is down, receives a proposal from Node2 with something different? * the protocol fails * it promises because the round number is higher than the previous proposal, but it also adds the info of the previous one * it doesn't promise because it already had a previous promise * 2
What is log replication in Raft? * The act of sending hearthbeats * The act of sending log infos with hearthbeats * Building a full log * 2
What happpens if a network partition happens in Raft? * The partition without leader stops to wait the one with the leader * The protocol fails * The partition without leader starts a new vote * 3
What happens when the network partition is solved in Raft? * The leader of the portion that went on with higher numbers will be the new leader * The original leader is restored * A new leader election starts * 1
After network partition, what happens in Raft if the two partitions went both on and have the same number? * Choose depending of who is the previous leader * Another leader election * Choose based on the log with most entries * 3
Which one isn't part of a fail-stop scenario? * computers can take for arbitrary time * one-to-n msgs possibile * msgs can be lost * 2
What is a leader-initiated log-storing log compaction? * A snapshot taken by the leader and stored externally * A snapshot taken by the application and stored externally * A list of checkpoint stored directly on the log * 3 
What is leader-initiated externally-storing log compaction? * A snapshot taken by the leader and stored externally * A list of checkpoint stored directly on the log *A snapshot taken by the application and stored externally * 1
What is indipendently-initiated externally storing log compation? * A snapshot taken by the leader and stored externally * A snapshot taken by the application and stored externally *  A list of checkpoint stored directly on the log * 2
Which are the building blocks of Zoekeeper? * Wait-Free, Ordering, Locks * Wait-Free, Ordering, Change Events * Blocking, Ordering, Change Events * 2
Zoekeeper's ordering means: * writes are linearizable and reads have ACID's isolation * writes are linearizable and reads are serializable * both means the same * 3
Zoekeeper's ordering is with * LIFO * FIFO * both of them * 2
Zoekeeper's Change Event implies that * changes gets notified before the result can be accessed * the opposite * depends * 1
Zoekeeper works like * filesystem for small pieces of data + notify - partial r/w * single point of failure * filesystem for big data + notify - partial r/w * 1
Faster network aka nearer nodes means more failures? * Yes, it's a trade off between space and latency * Yes, it's a trade off between reliability and latency * No * 2
Will Zookeeper become faster or slower by adding new nodes to the cluster? * Slower * Faster for reads, slower for writes * Faster for writes slower for reads * 2
Which consensus Zookeeper uses? * Raft * Similar to Raft, hidden internally* Paxos * 2
Which of these isn't something to take into account for choosing the number of znodes? Performance (of r/w), Costs, Tolerable failures (avaibility) * performance * avaibility * none * 3
Ephimeral node means: * if the creator of this node (host) fails, znodes is deleted * all the children of it are deleted * after some time are deleted * 1
What you have to check if u want to put an x-lock? * Whatever is doing the node before you * The X-lock before you * The S-lock before you * 1
What you have to check if u want to put an s-lock? * Whatever is doing the node before you * The X-lock before you * The S-lock before you * 2
In Zookeeper, how is a write performed? * directly write to leader * write to a node, node will ask leader to add the write and give ack only when the leader say that the write propagation is done * write to a node, node will propagate to everyone and then ack * 2
What is the key inspiration behind the design of Apache Zookeeper? * The implementation of the NFS filesystem * The need for order and sequencing in data processing * The coordination of distributed applications * 1
Which data structure does Apache Zookeeper primarily utilize for organization? * Trees * Graphs * Hashmaps * 1
How is the leader elected in Apache Zookeeper? * By proposing oneself as a candidate * By consensus of a majority of servers * By monitoring changes in the system * 1
What does the 'Sequence' flag ensure in the Apache Zookeeper data model? * Append a counter to data * Monitor leader changes * Maintain consistency across servers * 1
How is the update committed in Apache Zookeeper? * When all servers have saved the change * When a majority of servers have saved the change * When the leader has approved the change * 2
What was Google's desired approach to transaction processing? * External consistency * Internal consistency * Sequential consistency * 1
What is the main problem with the exclusive lock approach in a large system? * It allows only one read operation at a time * It prevents any other operations while writing * It slows down the system significantly * 2
What is the key feature required for tracking historical data in the system? * Low latency clocks * Super-precise clocks * Atomic clocks * 2
What does the TrueTime.now() function return in the Google Spanner system? * A single timestamp value * An interval between earliest and latest times * The average time across all datacenters * 2
What information is extracted after repeated measurement in the Spanner system? * Statistical averages * Time offset and round-trip delay * Error intervals * 2
What is the purpose of the algorithm used to weed out inaccurate time masters in the Spanner system? * To identify the most consistent time interval * To calculate the average time across all intervals * To prevent delays in the system * 1
What was the primary issue with utilizing Paxos for every update in Google's distributed datacenters? * High network latency * Overloading of servers * Inefficiency in processing updates * 1
After the clock is synchronized, the uncertainty grows according to: * best-case assumptions on computers’ clock drift * worst-case ones * exponential growth * 2
How is the clock drift handled? * Doesn't matter because freq(clock drift) << freq(cpu fails) * Using atomic clocks * re-sync every 30 seconds. The first one is when clocks broke down * 3
In the TrueTime example, what happens during a network congestion? * spikes in red part * 99.9 percentile threshold decreases * 99.9 percentile threshold increases * 3
What is the key feature of the data model used in GoogleCS? * Hierarchical storage * Key-value store * Relational database * 2
How are conflicting transactions handled in GoogleCS? * They are executed in parallel * They are assigned timestamps based on their completion * They are given disjoint locking intervals * 3
What is the purpose of using TrueTime in GoogleCS for assigning timestamps? * To prevent transaction errors * To ensure faster processing of transactions * To handle uncertainty in the clock * 3
How are non-conflicting transactions timestamped in GoogleCS? * Based on their completion time * Based on their start time * Based on their order of execution * 2
What is the role of Paxos in GoogleCS's operation? * Assigning timestamps to transactions * Handling intercontinental consensus * Managing data conflicts * 2
What determines the timestamp used for assigning writes in GoogleCS? * The moment of transaction completion * The moment of lock acquisition * The moment of transaction initiation * 2
How does GoogleCS handle large clock uncertainty in relation to transaction length? * It slows down transaction processing * It increases the system's processing speed * It extends the transaction length * 1
What does λ represent in the context of the M/M/1 queue model? * Arrival rate of jobs * Service rate of the server * Time spent in the queue * 1
What is the condition for the queue to be in equilibrium in the M/M/1 model? * λ > μ * λ < μ * λ = μ * 2
What does Little's Law in the M/M/1 model define? * The average time spent in the queue * The average queue length * The probability distribution of jobs * 1
What is the average time spent in the system for an M/M/1 FIFO queue according to the provided text? * λ * (1 - λ) * 1/(1 - λ) * 3
What is the key property of the M/M/1 queue model as described in the text? * Jobs arrive and are served in a memoryless fashion * Jobs are served based on their priority * Jobs are served sequentially * 1
According to Little's Law, what does L represent in the context of the M/M/1 model? * Average time spent in the system * Average queue length * Arrival rate of jobs * 2
What is the average time spent in the system for an M/M/1 FIFO queue when the arrival rate (λ) is 0.5? * 1 * 2 * 0.5 * 2
Leave when (wait till) TT.now().earliest > ? * TT.now().latest chosen at lock acquisition * ts * they're equal * 3
Avg length of the queue is? * 1/(1-λ) * λ/(1-λ) * (1-λ)^x * 2
In queueing, if λ = 0.9 i have * 9 jobs before me * w = 10 * both are related * 3
In a multi-server system, what is the potential advantage of assigning jobs to the least loaded server? * Better load balancing * Faster processing * Reduced latency * 1
According to Mitzenmacher's findings, what happens to the fraction of queues with at least I jobs when only a small number of servers are queried? * It increases * It decreases * It remains constant * 2
What is a characteristic of preemptive policies in a system with a mix of small and large jobs? * Prioritization of small jobs * Random processing order * No interruption of running jobs * 1
Which scheduling technique can minimize the average time spent on processing in a system where job sizes are known? * FIFO * Shortest Remaining Processing Time (SRPT) * Round-robin scheduling * 2
What potential issue may arise if job size information is incorrect in the SRPT scheduling policy? * System blocking * Increased processing time * Data corruption * 1
What is a drawback of relying solely on mathematical models for evaluating distributed systems? * Overemphasis on real-world problems * Lack of precision * Overemphasis on implementation * 2
What is one advantage of conducting experiments and measurements on real systems for evaluation purposes? * Lower costs * Scalability * Detailed insights * 3
Which evaluation method allows for scalable and cost-effective analysis of distributed systems? * Experiments * Simulations * Mathematical models * 2
What technique can be used to schedule tasks in a system with multiple servers, prioritizing machines with the fewest jobs in queue? * FIFO * Preemption * Supermarket technique * 3
When estimating job sizes, what is the suggested approach to avoid potential system blockage due to incorrect information? * Prioritize large jobs * Prioritize small jobs * Balance job sizes * 2
How can we proove that SRPT is better? * By prooving that I can always improve something different * Mathematically * By prooving that there's no starvation * 1
If I have C0 and C1 as completed times of P0 and P1, what's W roughly? * No point in computing it * I sum the two contributions and I divide by 2 * (C1 - C0) / 2 * 2
What is the main objective of Erasure Coding in a distributed computing system? * Minimizing data loss * Maximizing data redundancy * Reducing storage costs * 3
How does the Trivial Solution of data replication work in safeguarding against server failures? * Creating extra data copies * Applying XOR operations * Utilizing linear equations * 1
In the most basic scenario, what's the calculated redundancy with M=2? * 0.5 * 3 * 2 * 2
What is the redundancy value in a scenario with N=6 and M=1, using the general Parity technique? * 0.5 * 1 * 1.2 * 3
What is the redundancy value in a scenario with N=6 and M=2, using the linear oversampling technique? * 0.5 * 1.5 * 1.2 * 2
What do any K distinct points represent in the Polynomial Oversampling technique? * A single polynomial of degree K-1 * Multiple linear equations * Data encryption keys * 1
In the context of Polynomial Oversampling, what happens to the size of the numbers in the encoded message compared to the original data? * They remain the same * They decrease, can lead to underflow * They become larger, can lead to overflow * 3
What are: Associativity, Commutativity, Additive/Multiplicative identity, Distributivity, Add/Mult inverses? * properties of general fields * properties of galois fields * properties of inverse modulo fields * 1
Why Finite fields are cool? * Because they have associative properties * because they're finite * for log2(n) encoding * 3
What is the outcome when computing the multiplicative inverse in a modulo p finite field? * A unique value for each element * No multiplicative inverse * Multiple inverse values * 1
What does the multiplication table modulo m represent in the context of finite fields? * Addition and subtraction rules * Division operations * Multiplication and division operations * 3
What is the reason behind the proof that there is exactly one multiplicative inverse in modulo p? * Prime numbers properties * Multiplicative identity * Field distribution rules * 1
What can I do in Erasure Coding if the encoded msg gets bigger when multiplying? * truncate * fields * Galois fields * 3
Given c0 + c1x + c2x^2 + c3x^3 (mod 7) and f(0)=3, f(2)=1 I have: * c0=3, 1 = 3 + 2c1 + 4c2 + 8c3 * c0=3, 1 = 3 + 2c1 + 4c2 + c3 * c0=3, 1 = 3 + 2c1 + 4c2 * 2
Given 5c1 + 6c2 = 0... * 5c1 = -6c2 -> 5c1 = c2 -> (mult by inv both sides) c1 = 2c2 * 5c1 = -6c2 -> 5c1 = c2 -> (mult by inv both sides) c1 = c2 * 5c1 = -6c2 -> 5c1 = c2 -> (mult by inv both sides) c1 = 3c2 * 3
In practical scenarios, how is data treated for storage across multiple machines? * Divided in a sequence of K blocks, encoded in series of values smaller than p  * All elements of the first (original) block will be c0 coefficients and so on * both * 3
Complete the ?: N is |machines|, M is |?|, choose p, divide in ?, encode as ?, encode again and put all the ? and so on * tolerable failures, K blocks of same size, series of values < p, y0 coeff in the fst block * tolerable failures, K blocks of same size, series of values < p, c0 coeff in the fst block * both * 1
What is recommended regarding the choice of p in the encoding process? * Choose p to be smaller than N * Ensure p is not smaller than N * Set p equal to M * 2
In coding theory, what are the finite fields typically used by computer professionals? * Fields with 2^m elements * Fields with p^m elements, where m ≥ 1 * Fields with m elements * 1
What is a characteristic feature of fountain codes in comparison to other coding approaches? * They require a fixed number of redundant blocks * They do not need a predetermined N to function * They operate efficiently for small data sets * 2
What is the key advantage of regenerating codes in data recovery scenarios? * They require the original plain-text to recreate lost blocks * They can regenerate lost blocks without external information * They ensure the integrity of the original data * 2
What is the purpose of padding in the encoding process? * To reduce the number of coefficients * To ensure the number of values is a multiple of K * To adjust the size of the data blocks * 2
What is the approximate space requirement for the storage and reconstruction of 100 GB of data using advanced theory? * A bit more than 100 GB * Less than 100 GB * Exactly 100 GB * 1
What was the original purpose of the Tor project when it was initially funded by the US government? * Protecting intelligence communication online * Facilitating secure online transactions * Ensuring freedom of speech on the internet * 1
What is a key security assumption for the effective use of the Tor network? * All routers should be owned by the same entity * Attackers have access to all traffic from both guard and exit nodes * Routers can decrypt messages peeling one layer * 3
Which is the riskiest node in Tor? * Starting Node * Exit Node * both * 2
What are the Security Assumptions in Tor? * All routers choose shouldn't be owned by the same attacker * Attacker can't see your traffic from both guard and exit nodes * both * 3
What security protocol does Tor use for encapsulating TCP connections? * UDP * TCP * SOCKS * 3
Why is using BitTorrent over Tor not recommended? * BitTorrent uses UDP connections (all the Tor work isn't worthy bcs one UDP packet isn't reliable) * BitTorrent requires specialized hardware that Tor doesn't support * It can expose user data to potential attackers * 1
Why is using Tor on an everyday browser not recommended? * It doesn't support certain types of data transmission * It can lead to correlation of website visits through cookies and fingerprints * It hinders access to specific websites * 2
How is the Tor Browser designed to prevent tracking and correlation of user activities? * It uses advanced encryption algorithms * It blocks all cookies and website data * It is designed to look identical for all users and always operates through the proxy * 3
What is the main purpose of bridges in the Tor network? * Bridges are used to control the traffic flow within the network * Bridges prevent unauthorized access to the Tor network * Bridges allow people to access Tor nodes in countries where they are blocked * 3
Which messages are encypted by the Tor protocol? * The first communication with the directory server * That + all the others except the one after the Exit Node * All of them * 2
If I want to start a Tor service I pick some .. points and I build .. between them (so Guard, .. and Exit nodes) * Introduction, links, Relay * Service, circuits, Relay * Introduction, circuits, Relay * 3
What happens after B advertises his service onto the DDB and A gets these info? * Sets a rendez-vous point and then writes a PK-encrypted listing of that points + a one-time secret * Then asks about an intro point for sending that msg to B, that will retrieve the secret from the rendez-vous point * first and then second one * 3
How can bridges be discovered and censored? * Through deep packet inspection (DPI) * By scanning IPv4 addresses * both * 3
What is the main countermeasure to discovering bridges in the Tor network? * Sybil attack detection * Deep packet inspection * Obfuscation, hiding protocol to DPI * 3
What percentage of Tor bridges were discovered through a complete scan of all IPv4 addresses in 2013, as per Durumeric et al.? * 56% * 86% * 100% * 2
What is the real-world problem in website fingerprinting within the context of the Tor network? * Fixed message sizes * Higher latencies * Lot and changing websites * 3
What is the main factor that makes website fingerprinting less efficient in the Tor network? * Varying website sizes * Higher latency and fixed 512B msgs (cells) * both * 2
What's a technique to identify which website a user is looking at by looking at the sizes and timing of encrypted packages * DPI * Website Fingerprint * pluggable transport * 2
What's the attack in which an assailant creates a large number of nodes to subvert the system, increasing likelihood of correlating traffic? * Sybil Attack * NSA Attack * Deep Packet Inspection * 1
What does "Tor Stinks" refer to in the context of NSA's perspective on Tor's security? * Limited success in de-anonymizing Tor users and attacking controlling nodes and exploiting vulnerabilities * Flaws in the Tor network that made a lot of Sybil attacks possible * A successful attack on Tor * 1
In Sybil Attack with respect to Tor nodes, replicating nodes makes possible for the Attacker to have access to both Guard and Exit Nodes. What's a countermeasure? * fingerprint node behavior (joining, uptime, ..) * DPI * crypting with PUB key * 1
What was the primary function of the Napster central index server in its file-sharing service? * Routing messages to peers * Aggregating query routing tables * Storing information about available songs * 3
What was the primary motivation behind creating overlay networks in P2P systems? * To ensure centralized control * To enable copyright protection * To provide scalability and fault tolerance * 3
How did the Gnutella network handle new query requests initially? * By directly forwarding queries to all neighbors * By routing queries through a central server * By limiting the number of queries sent * 1
Which application marked the transition to completely decentralized P2P applications following the demise of Napster after its legal problems? * Spotify * Amazon Dynamo * Gnutella * 3
What has an overlay network where each node is connected to a few others + Bootstrap: each node contacts some services (“caches”) to get some nodes to connect with? * Gnutella * Amazon Dynamo * Napster * 1
What was the maximum TTL (Time-to-Live) value set for new queries in the Gnutella network? * always 4 * 6, then 4 * 7, then 4 * 3
How many queries in flooding method for Gnutella searching, that leaded to overloading (before ultrapeers connected with 32+ nodes)? * sum from i = 0 to tte of (5 times 4^i) * of (7 times 4^i) * of (4 times 4^i) * 1
What did the introduction of Ultrapeers (separated from leaf ones) in Gnutella help address? * Network security concerns * Bandwidth overload for dial-up nodes * Copyright infringement issues * 2
What information do Ultrapeers, connected with 32+ peers while leaves only with 3, send to their neighbors in Gnutella (only if there's a hope in them having the file)? * Query results * Query routing tables (representation of files that they have) * File availability status (with corresponding probability) * 2
What are the two main methods associated with Bloom Filters? * test(x) and add(x) * find(x) and insert(x) * search(x) and remove(x) * 1
When using Bloom Filters, what is the trade-off made regarding the possibility of false positives? * Ensuring a high probability of false negatives * Reducing the space required for data storage * Increasing the likelihood of false negatives * 2
How many hash functions are typically used in Bloom Filters to map elements to values in a range? * Just one * K different hash functions * K, generable with hi(x) = h(i || x) * 3
What happens when a Bloom Filter test for an element returns all corresponding bits as 1? * A false positive is identified * A false negative is identified * The element could successfully be retrieved only in this case * 3
If my query is “foo bar” what do I want in Bloom Filters? * get an answers for files that match with both keywords * we want QRTs to be as small as possible * both * 3
In Bloom Filters, p(an arbitrary bit is 0) = ? * (1 - 1/m)^(k) * (1 - 1/m)^(nk) * (1 - 1/m)^(n) * 2
In Bloom Filters, p(an arbitrary bit is 1) = ? * [(1 - 1/m)^(k)] - 1 * (1 - 1/m)^(nk) * 1 - [(1 - 1/m)^(nk)] * 3
In Bloom Filters, p(false positive) = ? * (p(bit is 1))^k * {1 - [(1 - 1/m)^(nk)]}^k * both * 3
In Bloom Filters, approx(false positive) = ? * [1 - (1/e)^(nk/m)]^k  * [1 - (1/e)^(nk)]^k * [1 - (1/e)^(nkm)]^k * 1
What do I obtain putting the derivative of approx(false positive) = 0 ? * optimal k = (mln(2))/n * for the previous k I obtain that p(an arbitrary bit is 1) = 1/2 (GOOD for entropy!) * both are right and we obtain that best is when it's half full and half empty * 3
If we fix p(false positive) = eps we have: * m = -(nln(eps)/(ln(2)^2) AND, for an error rate of 10%, eps = 0.1 and log10(eps) = −1 we have 15 bits per item! * same as before but with 4.79 bits per item * same but m is wrong because ln(eps) = -(m(ln(2)^2)/n * 2
What advantage do Bloom Filters offer in the context of databases? * Efficient caching mechanism in RAM * Streamlining access to external disks * Minimizing false negatives in disk searches * 1
How do Bloom Filters aid in the optimization of web caches? * Avoid storing data requested only once * By accelerating data retrieval from external sources * By reducing the occurrence of false positives * 1
why in Bloom Filter the best k is a little bit higher than expected? * due to overflows * due to when it finds already placed ones * due to integer approximation * 2
The fingerprint of x is placed into one of buckets h1(x) and h2(x). If the buckets are full, then one of the fingerprints in the bucket * is deleted from the cuckoo filter * is evicted using cuckoo hashing and placed into the other bucket where it can go (and so on) * is evicted using cuckoo hashing and placed in a h3 bucket * 2
Apart from space efficiency, what capability does a cuckoo filter possess that Bloom filters do not? * Deletion of existing items * Efficient item insertion * Low false positive rates * 1
What key advantage does Distributed Hash Table (DHT) offer? * Improved fault tolerance * Enhanced key-value lookups * Efficient resource allocation * 2
What is the primary role of Consistent Hashing in the context of DHTs? * adding or removing peers has a small impact on resource allocation * Perfect to handle churn: nodes arriving and leaving all the time * both 1) and 2) * 3
In the Chord Ring, how is item x stored on a node? * Corresponding to a random identifier * Matching its specific hash value * Determined by the node's predecessor * 2
How does Chord achieve accelerated lookup in its routing mechanism? * By implementing random node connections * Through the creation of exponential shortcuts * By establishing direct links to all successor nodes * 2
In Chord's greedy routing, what approach is taken at each step of the lookup process? * Following the closest, but before, finger to the destination * Prioritizing communication with all successor nodes * Randomly selecting a successor at each step * 1
What is the minimum number of steps required for lookup in Chord, given n total nodes? * O(n)* O(nlogn) * O(log n) * 3
How does Chord ensure fault tolerance within the system? * By periodically redistributing data among all nodes * Through a comprehensive backup and recovery protocol * By maintaining a list of successor nodes and regular stabilization procedures * 3
What is the average path length for a system with n=4,096 nodes, according to the Chord architecture? * log2(n)/2=6 * log2(n) * n/2 * 1
Why the average path length resembles a gaussian? * because half of the time I'll take the before part of the exp result * because half of the time I'll take the after part of the exp result * both means the exact same thing lol * 3
What is the distinctive feature of the Kademlia DHT architecture that sets it apart from Chord? * Symmetrical links for info of who and when can reach you * Higher fault tolerance capabilities * Reduced average path length for lookup * 1
How does Kademlia handle the establishment of node connections based on common bits? * By creating multiple intermediary nodes for each connection * Through the utilization of logarithmic routing for each node * By establishing a finger table with specific bit matches in common (like Hamming) * 3
In what way does the Chord architecture handle the addition of new nodes to the system? * By a lookup followed by taking the right data from both previous and next node * By a lookup followed by taking the right data from next node, and telling the prev to update its cache * By initiating a comprehensive data exchange with multiple nodes * 2
What key issue does the fault tolerance mechanism in Chord aim to prevent? * Data loss due to node failures * Disruption of the successor chain when no links remain in a node * Excessive resource consumption when too much messages are exchanged * 2
What purpose does a BitTorrent tracker serve in the system? * Facilitating node discovery, but since 2005's DHT's is no more useful * Seeding the file * Facilitating node discovery * 1
How are files divided in the BitTorrent system by default? (a node doesn’t report having a piece until it verifies this sized hash) * 128KB * 256KB * 512KB * 2
What does the "rarest first" piece selection strategy in BitTorrent ensure? * High download speeds * Efficient resource allocation * Retention of rare pieces * 3
Apart from the "rarest first" strategy, what is the exception for the initial piece selection in BitTorrent? * Fastest first * Largest first * Random first * 3
What concept in game theory is the "Tit for Tat" strategy in BitTorrent inspired by? * Nash equilibrium * The prisoner's dilemma * The Monty Hall problem * 2
How does a node confirm that it possesses a specific piece in the BitTorrent system? * By checking its size in .torrent * By verifying its hash in .torrent * By contacting the tracker * 2
What is included in a .torrent file in the BitTorrent system? * name, size and hashes * name, owner and hashes * name, owner, tracker and hashes * 1
What are the two main types of nodes in the BitTorrent swarm? * Trackers (node with a full copy of the file, just uploading) and downloaders (nodes that have not finished downloading the file) * Seeds (node with a full copy of the file, just uploading) and downloaders (nodes that have not finished downloading the file) * Servers and clients * 2
How frequently does the optimistic unchoke strategy provide an upload slot to a random node in BitTorrent? * Every 10 seconds * Every 30 seconds * Every 60 seconds * 2
Which of the following options best characterizes the behavior of the "Tit for Tat" strategy? * Unpredictable reciprocation * Continuous unchoking * Cooperative retaliation * 3
What is BitTorrent? * file distribution system * alternative to HTTP and FTP * both * 3
In the context of BitTorrent, what does "choked" refer to? * Unrestricted connection * Restricted upload (uploaders are not sending data through them) * Restricted download (downloaders are not receiving data through them) * 2
How many connections are left unchoked by default in the BitTorrent system? * 2 * 4 * 6 * 2
What is the purpose of the "optimistic unchoking" strategy in BitTorrent? * To hope for the discovery of new, potentially faster connections.* To limit data congestion * To restrict data uploads * 1
What prevents fake file sharing in Bit Torrent? * You download hash by hash (not partially) and you check the match with .torrent file * You'd lose download speed due to behavioural reports * both * 1
What are these systems based on? Paxos or Raft based, Eventual Consistency based * CP, CA * CP, AP * CA, CP * 2
In the 1970s, what was a key characteristic of the Relational approach to data management? * Bottom-up development * Imperative language * ACID transactions * 3
What BASE stands for? * Basically Available, Soft State, Eventual Consistency * Biased Acid Sium Enterprise * Basically Available, Soon or Later, Eventual Consistency * 1
What is the purpose, after Partition Mode, of Partition Recovery? * Get somehow back to a consistent state * Detect and repair system mistakes * both * 3
How do ATMs limit damage during "Stand In" time? 1)Detect Errors 2)? * Decrease transaction limits * Increase network connectivity * Implement overdraft penalties * 3
In eventually consistent systems, what are the three key issues that need to be addressed? * Detect, define partition mode, and recover from partitions * Implement, detect, and compensate for errors * Define partition mode, detect, and monitor partition modes * 1
What was Dynamo (originally a key-value store as available as possible) designed for? * Data storage * Shopping cart handling * Streamlining logistics * 2
Dynamo is always writbale, has user-perceived consistency and 99.9 latency, and you have to tune: * cost, consistency, durability and latency * cost, coherence, durability and latency * cost, coherence, durability and avaibility * 1
What does the "Key Idea" in Dynamo's architecture resemble in terms of its structure? * Binary tree with failure detection * Chord in a datacenter but with full partition table and gossiping * Chained linked list with one-hop only per data * 2
In which system I can directly donwload the full routing table as the Client? * Chord * Dynamo * Kademlia * 2
Which technique is used by Dynamo to ensure faster partitioning when nodes join or leave the network? * Splitting the hashing space in Q partitions: when nodes join they steal partitions from other nodes, when they leave they're redistributed * The same but they don' steal from other nodes * The same but they aren't initially equally distributed * 1
What does the API function "get(key)" return in Dynamo? * Single value and version info (passed to subsequent puts to solve inconsistencies) * Possible multiple values and version info * A list of values and their sources * 2
What are the three configurable parameters of the "Sloppy Quorum" flexible R/W system in Dynamo? * S, T, U * R, W, N * X, Y, Z * 2
If we have R + W > N in Dynamo we have: * Eventual Consistency * High consistency as a consensus algo * |reads| to get a successful read + |writes| to get a successfull write < number of copies for each piece of data * 2
Connect configurations in Dynamo: N=3, R=2, W=2 & N=x, R=1, W=x & N=R=W=1 * consistent and durable, slow writes and fast reads, cache * consistent and durable, cache, slow writes and fast reads * cache, consistent and durable, slow writes and fast reads * 1
What happens in Vector Clocks if version_info's counter of passed-through machines can't tell if a node supersedes another? * we have a failure * random choice if they're equal * they’re independent and we ask the client what to do * 3
What happens when a machine goes offline in Dynamo? Remember that it'ss considered transient, and permanent addition or removal is an administrator action * R/W spill over to the first machine in the ring after the N that should handle them by default * When the machine comes back online, updates are reported to it * both * 3
What is the purpose of "Client-Side Reconciliation" in Dynamo? * Load balancing across the network * Letting the app deciding how to handle multiple values returned * Rerouting operations to faster nodes * 2
What are buffered writes, throttling background operations and additional load balancing coordinating R/W's to fastest nodes * Dynamo Optimizations * Dynamo Problems * Dynamo pros * 1
How to spot differences & reconcile them with Anty Entropy in "Merkle Trees"? * Compare data between nodes that store replicas of a partition. Note that Hash(0) = H(H(0-0)+H(0-1)) * If the root is different, compare the children to find out which half is different, recursively * both are true stuff * 3
In Dynamo, what's the pro of only data that falls within the specific (N) range of the affected node needs to be transferred (preference list) * transferring partitions doesn't require random disk accesses * transferring partitions leverage random disk accesses * both * 1
Which are the differences between Dynamo and Cassandra? * Cassandra uses Zookeeper for routing table and seeds, and to elect the leader * Cassandra has vector clocks * Dynamo lightly-loaded nodes get migratted on the ring * 1
According to the material, what are the three defining characteristics of Big Data referred to as the "3 Vs"? * Value, Variety, Veracity * Volume, Velocity, Variety * Value, Velocity, Variability * 2
What is the primary reason behind utilizing a "Shared Nothing architecture" in the context of MapReduce? * Cost efficiency * Synchronization speed * Independent entities, with no common state * 3
What is one of the key principles associated with the use of MapReduce in data processing? * Scaling up, not out * Avoiding synchronization * Moving processing closer to data * 3
In the context of disk I/O, what is the primary advantage of organizing computation for sequential reads? * Reduced latency * Faster processing speed * Minimized seek latency * 3
In terms of scalability goals, what happens to the execution time of the same algorithm when the cluster size is doubled, according to the ideal scenario described in the material? * It remains the same * It is reduced by half * It is doubled * 2
What kind of processing does the MapReduce programming model excel at, involving (mostly) full scans of a dataset? * Real-time analytics * Stream processing * Batch processing * 3
What is the primary characteristic of the MapReduce framework's "Map Phase," as described in the material? * Aggregation operation * Data transformation * Key-value pair creation * 2
What is the primary purpose of the "Shuffle Phase" in the MapReduce framework? * Data transformation * Data grouping by key * Network optimization * 2
What does the "Reduce Phase" in the MapReduce framework primarily involve? * Data transformation * Aggregation operation * Key-value pair creation * 2
What is a key feature of the "Combiners" in the context of the MapReduce framework? * Full data aggregation * Pre-aggregation of data * Network bandwidth reduction * 2
What is the main design principle behind HDFS (Hadoop Distributed FileSystem), and what was its inspiration? * The design principle is to move computation to the data * It was inspired by Amazon Web Services * It was inspired by Google's GFS (Google File System) * 3
How does HDFS handle large datasets, and what kind of workloads is it tailored to? * Large datasets are partitioned across multiple machines, and it is tailored to write-intensive workloads * Large datasets are stored on a single machine, and it is tailored to latency-sensitive workloads * Large datasets are partitioned across multiple machines, and it is tailored to read-intensive workloads * 3
What are HDFS blocks, and why are they designed to be large? * HDFS blocks are chunks in which big files are broken, and they are large to optimize seek times and ease metadata handling * HDFS blocks are chunks in which big files are broken, and they are large to reduce replication overhead * HDFS blocks are small units of data used for redundancy, and they are large to improve fault tolerance * 1
What are the main components of HDFS, and what roles do they play in the architecture? * NameNode keeps metadata, DataNode stores data, and Secondary NameNode acts as a backup * The main components are DataNode, JobTracker, and TaskTracker * DataNode keeps metadata, NameNode stores data, and Secondary NameNode acts as a backup * 1
How does HDFS ensure fault tolerance in the storage of data blocks? * By using erasure coding for data redundancy * By replicating data blocks across different machines * By compressing data blocks to save space * 2
What is the purpose of the NameNode in HDFS, and how does it manage metadata? * The NameNode manages data replication across the cluster * The NameNode manages metadata in RAM, including directory trees and block indices per file * The NameNode manages data storage and recovery in case of failures * 2
In the HDFS architecture, what is the role of the Secondary NameNode, and how does it assist in system recovery? * The Secondary NameNode is responsible for storing data blocks * The Secondary NameNode acts as a backup for the NameNode and helps recover the system when the primary NameNode is down * The Secondary NameNode is used for load balancing in the cluster * 2
When a client reads a file from HDFS, how does it determine the DataNodes from which to obtain data blocks, and what is the sorting order? * The client randomly selects DataNodes to obtain data blocks * The client always selects the DataNodes with the highest available bandwidth * The client gets block locations from the NameNode and sorts DataNodes by proximity, starting with the same node and then the same rack * 3
Explain the concept of "pipeline replication" in HDFS when a client writes a file. What is the default replication strategy, and what trade-off does it address? * Pipeline replication involves creating multiple copies of a file in a sequential manner. The default strategy is to replicate the first copy off-rack and the second copy in the same rack as the first. This strategy balances reliability and cluster bandwidth usage. * Pipeline replication involves creating multiple copies of a file in parallel. The default strategy is to replicate all copies in the same rack. This strategy maximizes data availability. * Pipeline replication involves creating one copy of a file at a time. The default strategy is to replicate all copies off-rack. This strategy minimizes network traffic * 1
How is the number of reduce tasks determined in Hadoop, and what can users do to handle skewed data in the reduce phase? * The number of reduce tasks is user-specified, and users can write custom partitioners to handle skewed data. * The number of reduce tasks is automatically determined by Hadoop, and users cannot modify it. * The number of reduce tasks is based on the number of map tasks, and users can increase it as needed. * 1
What is the primary priority criterion for the FIFO scheduler in Hadoop, and what is one drawback of this scheduler? * The primary priority criterion is job size. One drawback is that it may lead to resource wastage. * The primary priority criterion is job priority level. One drawback is that it doesn't consider job arrival time * The primary priority criterion is the arrival time of jobs. One drawback is that it can penalize small jobs waiting behind large ones * 3
How does the Fair scheduler prioritize jobs, and what is its goal regarding job progress? * The Fair scheduler gives precedence to active jobs with the least number of running tasks. Its goal is to have each job make roughly the same progress at any given time. * The Fair scheduler prioritizes jobs based on their submission time. Its goal is to complete high-priority jobs first. * The Fair scheduler prioritizes jobs with the largest input data size. Its goal is to maximize data processing efficiency. * 1
How does the Capacity scheduler ensure resource allocation in Hadoop, and what is its feature regarding resource elasticity? * The Capacity scheduler creates virtual clusters with dedicated resources for each queue. It allows unused resources from one queue to be utilized by another queue. * The Capacity scheduler allocates resources based on job priority levels. It does not support resource sharing between queues. * The Capacity scheduler allocates resources based on job size, with larger jobs getting more resources. It maintains separate clusters for each job. * 1
What does it mean that HDFS is a Throughput-Oriented sys? * it's designed to prioritize throughput (amount of data that can be processed or transferred in a given amount of time) * In applications where the speed of individual read/write operations is critical, HDFS might not be the ideal choice * both * 3
What happens if a task fails in Hadoop, and how many retry attempts are made by default? * If a task fails, it's retried an unlimited number of times until it succeeds * If a task fails, it's retried a few times (e.g., 4) by default. After that, the job is marked as failed. * If a task fails, it's retried once, and if it fails again, the job is marked as failed. * 2
How does Hadoop handle a task that hangs with no progress during execution, and what action is taken in such a case? * If a task hangs with no progress, it's killed and retried. * If a task hangs, it's paused until the issue is resolved, and then it continues execution. * If a task hangs, it's allowed to continue indefinitely to ensure eventual completion. * 1
What action does the scheduler take when a worker machine fails in Hadoop, and how can Zookeeper be used in case of scheduler failure? * When a worker machine fails, the scheduler notices the lack of heartbeats and removes it from the worker pool. * In case the scheduler fails, Zookeeper can be used to set up a backup scheduler and keep it updated. * both * 3
Why combiners are used before spilling to disk in Hadoop? * Because we want to operate on the circular buffer * Because R/W operations on disk are a bottleneck * both * 3
In Hadoop, why mappers don't delete data right after it's send to reducers? * Because we're running a distributed merge sort * Because Reduce can fail and in that case we re-need data * because data has to be saved and replicated on HDFS * 2
Local HDFS DataNode -> Input Split -> Map -> In-Memory circular buffer -> partition, sort, spill to disk -> merge on disk (Local Linux FS) is? * Map-Reduce process * Shuffle on Map Side * Shuffle on Reduce Side * 2
Remote FS -> HHTPTransfer fetching data from mappers -> merges -> reduce -> Output Split -> Local HDFS DataNode is? * Map-Reduce process * Shuffle on Map Side * Shuffle on Reduce Side * 3
What is the primary goal of Spark in terms of data processing? * In-memory processing * Disk-based processing * Hybrid processing * 1
Which framework was designed to address the limitations of MapReduce for multi-stage applications and interactive ad-hoc queries? * Apache Kafka * Apache Spark * Apache Hadoop * 2
How does Spark handle fault recovery for Resilient Distributed Datasets (RDDs)? * By replicating data on multiple nodes * By using a distributed database * By logging all operations and recomputing lost partitions * 3
In the context of Spark, what does "coarse-grained operations" refer to? * Operations that modify data at the byte level * Operations that process the entire RDD (Resilient Distributed Datasets) at once * Operations that involve complex calculations * 2
What does Spark use to recover lost partitions in case of a failure? * Replication of data on all nodes * Lineage (i.e dependencies) information and recomputation * Distributed backups * 2
What are some use cases where MapReduce is considered less than perfect? * Real-time data processing * Multi-stage applications (Graph processing and iterative machine learning) * Data archival and backup * 2
What do messages.persist() and .cache() in Spark? * Save things on disk (persist) and memory (cache) * Stop performing operations without saving them after it * both * 3
What does messages.filter(...).count() in Spark? * System stops being lazy and start aggregating, counting and saving on disk * System starts doing MapReduce * both * 1