An agent is an hw and sw system that is ? (lives in some env, affecting and being affected by it), ? (some decisional power), ? (reactive, proactive, social) * situated, autonomous, flexible * this is the strong definition * both sentencies are true * 1
In the strong agent definition we also have a ? approach * mentalistic * emotional * antropomorphic * 3
A multiagent system is a ? of agents where they may belong to different organizations with their rules. Agents has incomplete information or capabilities and may interact via some form of explicit communication or indirectly via the enviroment as a ? * list, whiteboard * community, blackboard * list, blackboard * 2
In MAS there is no ? system control, data is ? and computation is ? * local, decentralized, async * global, centralized, sync * global, decentralized, async * 3
What we do when the agent metaphor has already been applied for the eng steps coming before in the SW cycle, when the intelligence of the individual agent is more relevant than the emerging intelligence of the overall system, when complex patterns of interactions may be required, when the application to be developed is geographically distributed, decentralized and open? * We employ agents as the technology for implementing the sw itself * We employ standard methods as technology for implementing the sw itself * Neither * 1
What is a new approach to modeling systems and a third way of doing science in addition to deductive and inductive, infering from simulated data? * Agent-Based-Modelling-and-Simulation * Agent-Based-Computation-and-Simulation * Agent-Based-Modelling-and-Communication * 1
First delusions with ? (in order to satisfy a goal the agent has to formulate an entirely new plan for it) does not allow the implementation of reactive agents (calculative rationality) and it's often undecidable * MASs * expert systems * first-principle planning * 3
what are the differences between agents and objects? * autonomy, flexibility * objects do it for free; agents do it for money, objects can be reactive but it's not embedded in the paradigm * both sentencies are true * 3
In ? psychology human behaviour is predicted and explained through the attribution of attitudes such as believing, wanting, hoping, fearing (e.g he worked hard because he wanted to possess a phd): these attitudes are called ? notions * modern, attitudinal * modern, intentional * folk, intentional * 3
? logic languages are suitable for specifying agents as intentional systems, adding "necessity" and "possibly" connectives * FOL * modal * propositional * 2
In ? agents can dynamically take ? or contractor roles: an agent faced with a task first assesses whether it can be divided into concurrent subtasks, announces the transferable tasks and requests bids from capable nodes. Nodes respond with bids, detailing their ability to perform the tasks. The agent then collects these bids and assigns the task to the node with the best bid * Manager Net Protocol, manager * Contract Net Protocol, manager * Contract Net Protocol, agent * 2
Actors were proposed as universal primitives of concurrenct computation: are self-contained, interactive autonomous components that communicate by async msg passing. Primitives are: * create, send, become (change an actor local state) * init, send, change * init, send, become  * 1
In an agent ?, modules and their interactions define how sensor data and current internal agent state determine its actions and future internal state * system * architecture * set * 2
? architectures focus on rapid responses to environmental changes(sense-act), ? architectures prioritize long-term action planning based on fundamental goals (sense-plan-act), and hybrid architectures integrate both * reactive, deliberative * deliberative, dynamic * static, reactive * 1
Reactive agents have minimal internal ? representation, tightly couple perception and action, and operate under a behavior-based paradigm where intelligence arises from agent-environment interactions * world * state * both * 1
In the basic schema of a reactive architecture, an agent is composed by ? that feed functions associating actions with sensed events, and then these actions are implemented by ? * states, effectors * listeners, effectors * sensors, effectors * 3
Intelligence is an emergent property of certain complex sys and so can only be generated with explicit representations as in symbolic AI * true * false * true but it's not symbolic AI * 2
In behavior languages a ? architecture is a hierarchy of task-accomplishing behaviors with simple rules * multi-agent * subsumption * subscriptions * 2
Limitations of reactive agents (without env models) are the need of sufficient info available from ? env and difficulty to ? specific agents (e.g with many behaviours) * global, build * local, engineer * global, engineer * 2
? agent architecture uses an explicit symbolic ? of the world, makes decisions through logical reasoning based on pattern matching and symbolic manipulation, and follows the sense-plan-act problem-solving paradigm of classical AI planning systems * deliberative, model * reactive, representation * deliberative, design * 1
The deliberative approach faces challenges in dynamic environments due to the need for continuous updates to the symbolic world model during planning, requiring a representation language that is both expressive and computationally manageable, and while classical planning seeks complete, optimal solutions, it often incurs high computational costs, making quicker, sub-optimal reactions sometimes more effective * true * false * true but it's hybrid * 1
Despite being too specific and unsupported by formal theories, an example of hybrid architecture is: * automata * turing machine * markov chain * 2
symbolic methods are based on high-level symbolic (human-readable) representations of problems (e.g ML) while in sub-symbolic AI the phenomenon is implicitely learned from experience * true * false * true but ML is sub-symb * 3
In the BDI architecture the reasoner in the env has (pre-compiled unlike in first-principle-planning) plans, ? (instanciation of a plan), ? (non-conflicting, adopted, desires) and ? (symbols that can also be false and/or conflicting) * goals, intentions, desires * goals, intentions, beliefs * objectives, wills, desires * 2
1) reasoner checks if new ? are present (may be sensed from env or self-generated), 2) reasoner has a queue of changed things either in goals or beliefs: these changes trigger plans 3) reasoner picks an appliable triggered plan to become an ? * goals/beliefs, intention * goals, intention * beliefs, instantiation * 1
How to improve fib prolog function? * put assert(fib(N, F)) at the end * put asserta(fib(N, F)) at the end * both are fine * 2
Is this okay? abs(X, X) :- X >= 0. abs(X, -X) :- X < 0. * Yes * No, base case is missing * No, it should be abs(X, Y) :- X < 0, Y is -X * 3
Is this okay? maximum(X, Y, X) :- X >= Y. maximum(X, Y, Y) :- X < Y. * Yes * No, base case is missing * No, the first clause is enough * 1
Complete: max([X], [X]). max([X|T], Max) :- max(T, Max1), ?? * Max1 > X * maximum(Max1, X, Max). * maximum(Max, X, Max1). * 2
What's the base case of: my_last([_|T], X) :- my_last(T, X) * my_last([], []). * my_last([X], X). * my_last(X, X). * 2
Complete: last_but_one([X, _], X). ?? :- last_but_one([Y|T], X) * last_but_one([_,Y|T], X) * last_but_one([SIUM,Y|T], X) * they're the same * 3
Complete: nth_el([X|_], 1, X). nth_el([_|T], K, X) :- ?? * K > 1, K1 is K - 1 * nth_el(T, K1, X) * both * 3
Is this right? my_length([], 0). my_length([X|T], Len) :- Len is Len1 + 1, my_length(T, Len1). * yes * no, last two are inverted * no, the base case is wrong * 2
Complete: rev(L1, L2) :- rev_aux(L1, L2, []). rev_aux([], L2, L2) :- !. ?? * rev_aux([X|T], L2, Acc) :- rev_aux(T, L2, [X|Acc]). * rev_aux([X|T], L2, [X|Acc]) :- rev_aux(T, L2, Acc). * rev_aux([X|T], Acc, L2) :- rev_aux(T, [X|Acc], L2). * 1
p(1,q(3)) =.. X. * X = [p, 1, q(3)] ; * X = p + 1 + q(3) * both * 1
F = p, A = 2 * functor(p(1,q(3)), F, A). * functor(p(4, 2), F, A). * both * 3
functor(1, F, A). * F = 1 * A = 0 * both * 3
arg(2, foo(a,b), Arg). * foo(a, b) = 2 * Arg = b ; * both * 2
In AgentSpeak(L) syntax an agent-goal ag is composed by beliefs (bs ::= at1..atn, where each at ::= P(t1..tn)) and plans (ps ::= p1..pn), where each plan p ::= * te : ct <- h * has a triggering event (dynamic change in beliefs or goals), a context stating when p is applicable depending on the current beliefs (logical formulae) and the body of the plan with actions, goals and belief (u)pdates * both are true! * 3
In first-order logic, ? represent objects, ? define properties or relations between terms, ? are the simplest statements formed by applying predicates to terms, and ? are logical expressions built from atoms, operators, and quantifiers to represent complex statements * terms, predicates, atoms, formulas * atoms, predicates, terms, formulas * terms, formulas, atoms, predicates * 1
Communication, in general, is the intentional exchange of information brought about by the production and perception of signs drawn from a ?? system of conventional signs; 1953 it was thought that all sentences could be either true or false *  shared, after *  shared, before * known, after * 2
What assumes the intentionality of the sentence (intent of the speaker), so that each illocution can be described in terms of what it is attempting to do * Philosophical Investigations * How to Do Things with Words * Speech Act Theory * 3
Speech acts, such as informing, querying, commanding, promising, and acknowledging, are used by speakers to achieve their goals and require knowledge of the situation, language conventions, and the hearerâ€™s goals, knowledge, and rationality * Can you see the gold? is a Query * OK is an ack * both * 3
Communication involves stages where the speaker forms an intention, selects and utters words, and the hearer perceives, analyzes, disambiguates, and incorporates the intended meaning, with potential issues arising from insincerity, ambiguous utterances, ignition failures, or context misalignment * true * false * partially true * 1
What are: Applications, Abstract Architecture, Agent Communication, Agent Management, Agent Message Transport? * FIFA specs grouped by category * FIFA spects from bottom-up * both * 1
What are FIPA message structure parameters (P) and what are Communicative Acts (A): inform if, agree, performative, sender, query ref, ontology, request, language * AAPPAPAP *  AAPPAPPP *  AAPAAPAP * 1
?? is the action of accepting a previously submitted proposal to perform an action, and it can be formally expressed by means of modal logic * call of proposal * accept proposal * propose * 2
FIPA's agent communication model assumes that agents share a common ? for their domain, ensuring consistent symbol interpretation, with ontologies either explicitly stored and represented or implicitly embedded in the agents' software implementation. A ? service for a community of agents is specified for this purpose * env * ontology * knowledge * 2
The FIPA ?? enables the discovery, maintenance, and translation of public ontologies, responds to queries about ontology relationships, and aids agents in identifying shared ontologies, while FIPA ?? define various suitable languages for representing ALC message content * Ontology Agent, Content Language Specifications * Agent Service, CLS * OA, ACL manager * 1
How can I express FIPA Contract Net Interaction Protocol? * Sequence diagram * AUML specific diagrams * informal representation is more suitable * 2
In Jason, ? adds a belief or goal, ? denotes an achievement goal, and ? represents a plan's body or actions to execute * +, !, <- * ?, !, < * +!, !, <- * 1
ps(t1,..tn)[a1,..am] is an annotated ? where ai ae first-order terms and ai are annotations. All predicates in the belief base have a special annotation ?(si) that can be self, percept or AgID * belief, a * predicate, a * predicate, source * 3
In Jason ? is for strong negation while ? is a negation as failure and it means that an agent isn't able to demostrate that something is true * !, not * tilde, ! * tilde, not * 3
In Jason we can also have ? expressed by ? rules instead of facts (e.g if I have apples I have fruits, but also if I have pears I have fruits...) * rules, explicit * beliefs, implicit * rules, implicit * 2
Jason's ? actions are libraries of user-defined (or predef with empty library name) actions accessible by dot * internal * examples are .desire and .intend * both true sentences * 3
In Jason, there is no way to select another plan for a triggering event if the previous plan failed in the ?, otherwise backtracking it's possible if in the ? * context, body * body, context * body, goal * 2
