An agent is an hw and sw system that is ? (lives in some env, affecting and being affected by it), ? (some decisional power), ? (reactive, proactive, social) * situated, autonomous, flexible * this is the strong definition * both sentencies are true * 1
In the strong agent definition we also have a ? approach * mentalistic * emotional * antropomorphic * 3
A multiagent system is a ? of agents where they may belong to different organizations with their rules. Agents has incomplete information or capabilities and may interact via some form of explicit communication or indirectly via the enviroment as a ? * list, whiteboard * community, blackboard * list, blackboard * 2
In MAS there is no ? system control, data is ? and computation is ? * local, decentralized, async * global, centralized, sync * global, decentralized, async * 3
What we do when the agent metaphor has already been applied for the eng steps coming before in the SW cycle, when the intelligence of the individual agent is more relevant than the emerging intelligence of the overall system, when complex patterns of interactions may be required, when the application to be developed is geographically distributed, decentralized and open? * We employ agents as the technology for implementing the sw itself * We employ standard methods as technology for implementing the sw itself * Neither * 1
What is a new approach to modeling systems and a third way of doing science in addition to deductive and inductive, infering from simulated data? * Agent-Based-Modelling-and-Simulation * Agent-Based-Computation-and-Simulation * Agent-Based-Modelling-and-Communication * 1
First delusions with ? (in order to satisfy a goal the agent has to formulate an entirely new plan for it) does not allow the implementation of reactive agents (calculative rationality) and it's often undecidable * MASs * expert systems * first-principle planning * 3
what are the differences between agents and objects? * autonomy, flexibility * objects do it for free; agents do it for money, objects can be reactive but it's not embedded in the paradigm * both sentencies are true * 3
In ? psychology human behaviour is predicted and explained through the attribution of attitudes such as believing, wanting, hoping, fearing (e.g he worked hard because he wanted to possess a phd): these attitudes are called ? notions * modern, attitudinal * modern, intentional * folk, intentional * 3
? logic languages are suitable for specifying agents as intentional systems, adding "necessity" and "possibly" connectives * FOL * modal * propositional * 2
In ? agents can dynamically take ? or contractor roles: an agent faced with a task first assesses whether it can be divided into concurrent subtasks, announces the transferable tasks and requests bids from capable nodes. Nodes respond with bids, detailing their ability to perform the tasks. The agent then collects these bids and assigns the task to the node with the best bid * Manager Net Protocol, manager * Contract Net Protocol, manager * Contract Net Protocol, agent * 2
Actors were proposed as universal primitives of concurrenct computation: are self-contained, interactive autonomous components that communicate by async msg passing. Primitives are: * create, send, become (change an actor local state) * init, send, change * init, send, become  * 1
In an agent ?, modules and their interactions define how sensor data and current internal agent state determine its actions and future internal state * system * architecture * set * 2
? architectures focus on rapid responses to environmental changes(sense-act), ? architectures prioritize long-term action planning based on fundamental goals (sense-plan-act), and hybrid architectures integrate both * reactive, deliberative * deliberative, dynamic * static, reactive * 1
Reactive agents have minimal internal ? representation, tightly couple perception and action, and operate under a behavior-based paradigm where intelligence arises from agent-environment interactions * world * state * both * 1
In the basic schema of a reactive architecture, an agent is composed by ? that feed functions associating actions with sensed events, and then these actions are implemented by ? * states, effectors * listeners, effectors * sensors, effectors * 3
Intelligence is an emergent property of certain complex sys and so can only be generated with explicit representations as in symbolic AI * true * false * true but it's not symbolic AI * 2
In behavior languages a ? architecture is a hierarchy of task-accomplishing behaviors with simple rules * multi-agent * subsumption * subscriptions * 2
Limitations of reactive agents (without env models) are the need of sufficient info available from ? env and difficulty to ? specific agents (e.g with many behaviours) * global, build * local, engineer * global, engineer * 2
? agent architecture uses an explicit symbolic ? of the world, makes decisions through logical reasoning based on pattern matching and symbolic manipulation, and follows the sense-plan-act problem-solving paradigm of classical AI planning systems * deliberative, model * reactive, representation * deliberative, design * 1
The deliberative approach faces challenges in dynamic environments due to the need for continuous updates to the symbolic world model during planning, requiring a representation language that is both expressive and computationally manageable, and while classical planning seeks complete, optimal solutions, it often incurs high computational costs, making quicker, sub-optimal reactions sometimes more effective * true * false * true but it's hybrid * 1
Despite being too specific and unsupported by formal theories, an example of hybrid architecture is: * automata * turing machine * markov chain * 2
symbolic methods are based on high-level symbolic (human-readable) representations of problems (e.g ML) while in sub-symbolic AI the phenomenon is implicitely learned from experience * true * false * true but ML is sub-symb * 3
In the BDI architecture the reasoner in the env has (pre-compiled unlike in first-principle-planning) plans, ? (instanciation of a plan), ? (non-conflicting, adopted, desires) and ? (symbols that can also be false and/or conflicting) * goals, intentions, desires * goals, intentions, beliefs * objectives, wills, desires * 2
1) reasoner checks if new ? are present (may be sensed from env or self-generated), 2) reasoner has a queue of changed things either in goals or beliefs: these changes trigger plans 3) reasoner picks an appliable triggered plan to become an ? * goals/beliefs, intention * goals, intention * beliefs, instantiation * 1